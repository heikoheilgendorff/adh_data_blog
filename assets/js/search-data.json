{
  
    
        "post0": {
            "title": "Exploring the High Resolution Population Density Dataset from Meta 2 (draft)",
            "content": "import pandas as pd import geopandas as gpd import altair as alt alt.data_transformers.disable_max_rows() .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/geo%20data/population%20density/data%20science/python/tutorial/2022/10/05/explore_geodata.html",
            "relUrl": "/geo%20data/population%20density/data%20science/python/tutorial/2022/10/05/explore_geodata.html",
            "date": " • Oct 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Mapping Data in Python (draft)",
            "content": "",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/python/datascience/2022/10/04/Python_mapping_tutorial.html",
            "relUrl": "/python/datascience/2022/10/04/Python_mapping_tutorial.html",
            "date": " • Oct 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/2022/10/04/Language-translation-with-python.html",
            "relUrl": "/2022/10/04/Language-translation-with-python.html",
            "date": " • Oct 4, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Introducing Metabase (draft)",
            "content": "%%html &lt;iframe src=&quot;https://metabase.africadatahub.org/public/dashboard/0e7e7c08-1a78-4303-85cb-90e549865917&quot; frameborder=&quot;0&quot; width=&quot;900&quot; height=&quot;800&quot; allowtransparency &gt;&lt;/iframe&gt; .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/metabase/overview/2022/10/04/Introducing_metabase.html",
            "relUrl": "/metabase/overview/2022/10/04/Introducing_metabase.html",
            "date": " • Oct 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "How We Created the ADH Inflation Database (complete)",
            "content": "Introduction . The International Monetary Fund (IMF) provide a great resource on inflation: the IMF Consumer Price Index (CPI) database. This database receives inflation data from nations across the globe and combines it into a single unified table. This is naturally a massive undertaking, and as it requires the co-operation of every country that is represented in the database, it can sometimes take a little while for the latest data to be published. This delay leaves the database temporarily out of date, often by more than a month. As the African Data Hub, our mission is to provide data journalists, researchers, and the general data with up to date, accurate, African-based data. In service to this mission, we have undertaken to maintain an inflation database, specifically focused on African countries, using the IMF database as the key dataset, but updating it with data released by individual African nations, before this information makes its way to the IMF. Thus we are able to maintain one of the most up to date inflation datasets in the world based exclusively on African datasets. . Data sources . The data that we have used to create this database can be found here and the full list of data sources can be found here. We gather the latest inflation data from different African countries by visiting the relevant websites (typically the country&#39;s national statistics bureau), searching for the latest inflation statistics, and then downloading the data. This data is often contained in official reports, published in PDF format. This can make extracting and using the data tables contained within the reports difficult and time-consuming. On rare occasions, the reports are accompanied by a separate data table release in CSV or XLSX format, which is much easier to use. The first step in combining the data from the IMF and the various countries is to extract the relevant data and convert it all to the same format. . Extracting data from PDFs and combining it into one dataset . The one advantage about formal reports that have been published in PDF is that they are typically consistently structured, with only the content changing. Thus we were able to construct an automated process for extracting data from PDF tables that was unique to each country. The data was extracted using a python module (look out for a blog post on this process, coming soon!) called Tabula, then cleaned and arranged in a manner that was consistent across all datasets, before being stored as a CSV. We then took our latest ADH Inflation dataset, updated it with the latest IMF dataset, and updated it further with the data scraped from individual countries. . Posting data . Once the data has been combined, we save the final dataset as a CSV file and then post it on the ADH CKAN data repository here. We also push all of the source data and scraped CSV files for each country to our githb repo and then provide public access to this data via links in each country dataset on CKAN, found here. .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/ckan/data/overview/inflation/2022/10/04/Inflation-Database-Methodology.html",
            "relUrl": "/ckan/data/overview/inflation/2022/10/04/Inflation-Database-Methodology.html",
            "date": " • Oct 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Introducing the Humanitarian Data Exchange (complete)",
            "content": "Introduction . The Humanitarian Data Exchange (HDX for short) is an open, online platform for sharing data across countries and organisations. It was launched in 2014 with the aim of making humanitarian data easy to find and explore, however, with currently more than 20 000 datasets on the platform, it can be a little intimidating for new users and it is easy to get lost! The purpose of this blogpost is to provide an overview of what kind of data is available on the HDX platform, how to search for the data you want, how to access it and how we at ADH interact with HDX data. If you are already familiar with HDX and have some specific questions, you may want to dive straight into their FAQ. . The HDX platform runs on CKAN, an open source data management system that is used by hundreds of organisations around the world such as our ADH data repository, and various national governments including USA, Canada, Singapore, Australia and others. . When using CKAN, it is important to understand the difference between the system&#39;s definition of a dataset vs a resource. A dataset is a collection of related data resources, while a resource is a single file. It may be useful to think of a dataset as a folder on your computer and a resource as a file in that folder. . Types of Data Available on HDX . HDX data is available in a wide variety of formats, from spreadsheets (eg: XLSX, CSV) to geographic (eg: SHP, geoJSON) to image (eg: geoTIFF, PNG) and even documents like PDF, DOCX, and PPTX. Some datasets will include the same data in different formats under different resources. It is therefore not always necessary to download an entire dataset, but rather, only look for formats that you are comfortable with or are able to use. Different formats of data are also often different sizes. For a tutorial on exploring an HDX dataset with different formats of the same data, see here ENTER PUBLISHED LINK HERE. . How to access and search for data on HDX . Searching for datasets on HDX is done in two ways: by searching for terms that you type into the search bar found at the top of almost every page on HDX, and by filtering a list of search results. The HDX search webpage can be found here and a screenshot of the same is shown in figure 1. . Entering a search term causes HDX to look for matching terms in the titles, descriptions, locations and tags of a dataset. This is very similar to searching for something on Google. The resulting list of items can be further refined using the filter options on the left side of the search result. You can filter by location, tag, organisation, license and format as well as filtering for some special classes of datasets. . Once you have narrowed your search down to a manageable list of datasets, you need to click on one and see if it contains any resources that you wish to use. You can download any resource by clicking on the download button to the right of the resource. As noted above, pay attention to the format of the resource that you wish to download and ensure that you download it in the correct format. . . HDX have a suite of data visualisation tools that will enable you to explore many of their datasets. It is usually possible to then download the data that fed the visualisation. . How ADH uses HDX data . The African Data Hub has its own CKAN data repository. The main aim of the ADH data repository is to provide accurate, up-to-date data on African countries. This repository is described in this ENTER PUBLISHED LINK blog post. The ADH data repository contains many datasets that are hosted on the HDX platform. The HDX platform has a global focus, while ADH has an African focus, thus it is our intention to highlight HDX&#39;s African datasets and make them easier to find as our repository holds significantly fewer than 20 000 datasets. The HDX African datasets that we have on our repository can be found here. In most cases, we simply provide the URL to where the data is hosted on the HDX platform, as there is little sense in hosting and maintaining those datasets ourselves. . Conclusion . The Humanitarian Data Exchange is a brilliant global, opensource, online data respository with thousands of datasets covering a wide range of humanitarian and crisis-related topics. As a data journalist, it is well worth the effort to spend some time familiarising yourself with it. The African Data Hub has an opensource, online data repository with an African focus that is continually growing. If you are looking for data from African countries, it could be worth looking on the ADH data repository first before searching through HDX as we may have found it already. .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/hdx/overview/ckan/data/2022/08/31/HDX_Blogpost.html",
            "relUrl": "/hdx/overview/ckan/data/2022/08/31/HDX_Blogpost.html",
            "date": " • Aug 31, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Introducing the African Data Hub CKAN Repository (complete)",
            "content": "Introduction . The African Data Hub (ADH) seeks to support and promote quality data-driven journalism in Africa by providing newsrooms, researchers, and the general public with easy access to quality African data. African data is typically difficult to find, stored in unwieldy formats, and is often out of date. ADH is working to remedy this by actively seeking out interesting and useful African datasets, converting them to more easily accessible formats, updating and creating combined datasets where possible storing them all on our opensource, online, CKAN data repository. . CKAN is an open source data management system that is used by hundreds of organisations around the world including the national governments of USA, Canada, Singapore, Australia and others. We use this resource to host any data we find that we believe may be useful in serving our mandate in the promotion of quality data-driven journalism in Africa. . Dataset vs Resource . When using CKAN, it is important to understand the difference between the system&#39;s definition of a dataset vs a resource. A dataset is a collection of related data resources, while a resource is a single file. It may be useful to think of a dataset as a folder on your computer and a resource as a file in that folder. When posting data on CKAN, you first need to create a dataset and fill in the required metadata. Then you can add resources either by uploading files, or linking them via URL. . Metadata . The importance of complete and correct metadata cannot be overstated. Metadata provides context for a given dataset which allows a potential user to understand what it is about, where it came from, how it was created and how it can be used. The following table provides details on the required metadata. . %%html &lt;h3&gt;&lt;b&gt;Table 1: &lt;/b&gt;Metadata description&lt;/h3&gt; &lt;p&gt; &lt;/p&gt; &lt;style type=&quot;text/css&quot;&gt; .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-j1i3{border-color:inherit;position:-webkit-sticky;position:sticky;text-align:left;top:-1px;vertical-align:top; will-change:transform} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} @media screen and (max-width: 767px) {.tg {width: auto !important;}.tg col {width: auto !important;}.tg-wrap {overflow-x: auto;-webkit-overflow-scrolling: touch;}}&lt;/style&gt; &lt;div class=&quot;tg-wrap&quot;&gt;&lt;table class=&quot;tg&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th class=&quot;tg-j1i3&quot;&gt;&lt;b&gt;Metadata&lt;/b&gt;&lt;/th&gt; &lt;th class=&quot;tg-j1i3&quot;&gt;&lt;b&gt;Description&lt;/b&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Title&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;A descriptive title&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Description&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Brief description of what is in the data - is this the industry standard, how does it compare to similar datasets, is this data hosted/used elsewhere? Summary of the methodology (include link to methodology where applicable). Link to example analysis (where applicable) &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Tags&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Main themes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Licence&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;&lt;b&gt;NB!!&lt;/b&gt; Licence this data is shared under. Use &lt;a href = &quot;https://chooser-beta.creativecommons.org/&quot;&gt; this resource &lt;/a&gt; if you&#39;re unsure &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Organisation&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;ADH organisation that sourced/uses this data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Visibility&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Can be set to public/private (ADH members only)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Source&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;&lt;b&gt;NB!!&lt;/b&gt; Link to where the data was found&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Version&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Version number&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Author&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Name of person/entity that produced the data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Author email&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Contact email for person/entity who produced the data&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Maintainer&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Name of ADH member responsible for this dataset&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Maintainer email&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Contact email for ADH member responsible for this dataset&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Groups&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Project this data is used for&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Data format&lt;/td&gt; &lt;td class=&quot;tg-0pky&quot;&gt;Filled in automatically&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/div&gt; . Table 1: Metadata description . . Metadata Description . Title | A descriptive title | . Description | Brief description of what is in the data - is this the industry standard, how does it compare to similar datasets, is this data hosted/used elsewhere? Summary of the methodology (include link to methodology where applicable). Link to example analysis (where applicable) | . Tags | Main themes | . Licence | NB!! Licence this data is shared under. Use this resource if you&#39;re unsure | . Organisation | ADH organisation that sourced/uses this data | . Visibility | Can be set to public/private (ADH members only) | . Source | NB!! Link to where the data was found | . Version | Version number | . Author | Name of person/entity that produced the data | . Author email | Contact email for person/entity who produced the data | . Maintainer | Name of ADH member responsible for this dataset | . Maintainer email | Contact email for ADH member responsible for this dataset | . Groups | Project this data is used for | . Data format | Filled in automatically | . Types of Data Available on the ADH data repository . The data is available in a wide variety of formats, from spreadsheets (eg: XLSX, CSV) to geographic (eg: SHP, geoJSON) to image (eg: geoTIFF, PNG) and even documents like PDF, etc. Some datasets will include the same data in different formats under different resources. It is therefore not always necessary to download an entire dataset, but rather, only look for formats that you are comfortable with or are able to use. Different formats of data are also often different sizes. For a tutorial on exploring a dataset with different formats of the same data, see here ENTER PUBLISHED LINK HERE. . Many countries tend to release their data in PDF format. Unfortunately, data in PDF form is usually difficult to work with. As such, when we come across PDF data that we believe could be useful for an African data journalist, we try to extract the data and present it in csv or xlsx format, which is much easier to work with. When ever we do this, we are sure to include links to the original PDF documents so that the authenticity and correctness of the extracted data can be verified by anyone who wishes to use it. . Finding and accessing data . Our data is organised in terms of datasets, organisations, groups and tags. Datasets have already been described above. . Organisations . Each organisation that is part of ADH is represented by an organisation on CKAN. Any data that was found, produced or used by a particular ADH partner can be found in that partner&#39;s organisation. . Groups . Groups indicate the data category, for example Health data, Economic data etc. A dataset may belong to more than one group. Groups also serve as folders for all datasets used in a particular data tool or project. Finally, if several different datasets have all been sourced from the same place, then all of those datasets are placed in a group named after that source. See for example, the Humanitarian Data Exchange, and the accompanying blog postENTER PUBLISHED LINK HERE. . Tags . Tags identify interesting characteristics of the data. For example, if the data contains gender information, it is tagged with gender, if it is geographical data, it is tagged with geodata. . Search . Searching for datasets on the ADH data repository is done in two ways: by searching for terms that you type into the search bar found at the top of almost every page on the ADH data repository, and by filtering a list of search results. Entering a search term causes CKAN to look for matching terms in the titles, descriptions, locations and tags of a dataset. This is very similar to searching for something on Google. The resulting list of items can be further refined using the filter options on the left side of the search result. You can filter by organisation, group, tag, format, and license. . Once you have narrowed your search down to a manageable list of datasets, you need to click on one and see if it contains any resources that you wish to use. You can download any resource by clicking on download on the dropdown explore button to the right of the resource. As noted above, pay attention to the format of the resource that you wish to download and ensure that you download it in the correct format. . Uploading Data . CKAN controls who can upload or make changes to existing data through user permissions. There are three levels of user permissions with administrator being the highest, followed by editor, and member. These permissions are assigned at both the organisation and group level. Only administrators and members are able to upload or edit data on CKAN. Members are able to view and download the data. There is also a system administrator who has control over the entire system. Visitors to the site, that is, people who do not have login credentials, are only able to view and download public data, while users with login credentials are able to view and download both public and private datasets. . Uploading data to the ADH CKAN platform is really easy. First, click on the datasets tab at the top of the screen and then click on the add datasets button as seen in Figure 1. . . You will be taken to a webpage like the one shown in Figure 2. Now fill in all of the metadata as described in Table 1. . . Once you have filled in all of the required metadata fields, click the Next: Add Data button near the bottom right of your screen as seen in Figure 3. . . Give your data file a descriptive name and a good description and then either upload your file or link it via URL. Click on Save &amp; add another if you have more files to upload, and when you have completed your upload, click on the Add button, seen here in Figure 4. . . If you need to edit or remove a file, click on the Manage button near the top right of your screen, as shown in Figure 5. . .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/adh/ckan/data/overview/2022/08/30/ADH_CKAN.html",
            "relUrl": "/adh/ckan/data/overview/2022/08/30/ADH_CKAN.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Exploring the High Resolution Population Density Dataset from Meta (draft)",
            "content": "The csv file is considerably smaller than the tif file, so let&#39;s begin with that. The first thing we need to do is convert it from a simple csv to geojson in order to be able to map it. . The tif file is very big - around 21GB when unzipped. This is too big for most machines to handle in Python, and you would be better off using a decidicated mapping tool like QGIS. As such, we should rather look to use the csv file, which is much smaller. However, first we should confirm that the data is the same. To do that, we select only a small part of the tif file and compare it with the same area in the csv file. . with rasterio.open(&#39;zip:///%szaf_women_2020_geotiff.zip!/zaf_women_2020.tif&#39;%data_path) as src: n = 500 data = src.read(1, window=Window(0, 0, n, n)) print(&#39;Band1 has shape&#39;, data.shape) height = data.shape[0] width = data.shape[1] cols, rows = np.meshgrid(np.arange(width), np.arange(height)) xs, ys = rasterio.transform.xy(src.transform, rows, cols) lons= np.array(xs) lats = np.array(ys) print(&#39;lons shape&#39;, lons.shape) . Band1 has shape (500, 500) lons shape (500, 500) . Now we have a subset of the tif file, but in order to be able to compare it with the csv file, we need to convert both into geopandas dataframes. To do that, we need to change the shape of the tif file&#39;s data. . lons2 = lons.flatten() lats2 = lats.flatten() data2 = data.flatten() data_df = pd.DataFrame({&#39;pop_dens&#39;:data2,&#39;latitude&#39;:lats2,&#39;longitude&#39;:lons2}).dropna() data_gdf = gpd.GeoDataFrame(data_df.pop_dens, geometry=gpd.points_from_xy(data_df.longitude, data_df.latitude, crs=&quot;EPSG:4326&quot;)) lon_min= data_df.longitude.min() lon_max = data_df.longitude.max() lat_min= data_df.latitude.min() lat_max = data_df.latitude.max() . gdf_box = gpd.read_file(&#39;%szaf_women_2020_csv_geo.geojson&#39;%data_path,bbox=(lon_min, lat_min, lon_max, lat_max)) . Right, let&#39;s take a look at the subset of the tif data. We can also zoom in to see that each point in the tif file is a square pixel. . plt.style.use(&quot;dark_background&quot;) data = np.nan_to_num(data) fg_color = &#39;white&#39; bg_color = &#39;black&#39; fig = plt.figure(figsize=(20,10)) ax1 = fig.add_subplot(121) plt.imshow(data) ax1.set_title(&#39;Plot of tif file&#39;, color=&#39;white&#39;) # set figure facecolor fig.patch.set_facecolor(&#39;black&#39;) # set tick and ticklabel color ax1.axes.tick_params(color=fg_color, labelcolor=fg_color) ax2 = fig.add_subplot(122) plt.imshow(data) plt.xlim(150,180) plt.ylim(180,150) ax2.set_title(&#39;Zoomed&#39;, color=&#39;white&#39;) # set tick and ticklabel color ax2.axes.tick_params(color=fg_color, labelcolor=fg_color) # set imshow outline for spine in ax1.axes.spines.values(): spine.set_edgecolor(fg_color) for spine in ax2.axes.spines.values(): spine.set_edgecolor(fg_color) fig.tight_layout() . Now let&#39;s compare the two geodataframes of the csv and tif subsets. Once again, we show a zoomed in image on the right. Bearing in mind that the tif data has been converted from a pixel to a point, we note that the two datasets place their points on opposite corners of the pixel. This is not a concern as both points represent the same pixel. . fg_color = &#39;white&#39; bg_color = &#39;black&#39; plt.style.use(&quot;dark_background&quot;) fig = plt.figure(figsize=(20,10)) ax1 = fig.add_subplot(121) gdf_box.plot(ax=ax1,label=&#39;csv&#39;) data_gdf.plot(ax=ax1,color=&#39;red&#39;,alpha=0.8, markersize=7,label=&#39;tif&#39;) ax1.set_title(&#39;Points from both datasets&#39;,color=fg_color) # set figure facecolor fig.patch.set_facecolor(&#39;black&#39;) # set tick and ticklabel color ax1.axes.tick_params(color=fg_color, labelcolor=fg_color) plt.legend() ax2 = fig.add_subplot(122) gdf_box.plot(ax=ax2,label=&#39;csv&#39;) data_gdf.plot(ax=ax2,color=&#39;red&#39;,alpha=0.8, markersize=7,label=&#39;tif&#39;) ax2.set_xlim(16.04,16.048) ax2.set_ylim(-22.050,-22.044) #ax2.set_ylim(-22.2044,-22.050) ax2.set_title(&#39;Zoomed&#39;,color=fg_color) # set tick and ticklabel color ax2.axes.tick_params(color=fg_color, labelcolor=fg_color) plt.legend() fig.tight_layout() . We have now determined that the csv and tif data refer to the same thing. As such, it is safe for us to continue our analysis using only the csv data. .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/geo%20data/population%20density/data%20science/python/tutorial/2022/08/12/geo-data-prep-tutorial.html",
            "relUrl": "/geo%20data/population%20density/data%20science/python/tutorial/2022/08/12/geo-data-prep-tutorial.html",
            "date": " • Aug 12, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Exploring the WHO's Malaria data (complete)",
            "content": "The World Health Organization (WHO) has some interesting datasets on Malaria. We have linked some of them in the ADH CKAN data repository. In this blog post, we provide a quicklook tool to showcase the information available in this data. This post is coded in Jupyter Notebook and if you are interested, you can view the code by clicking Show Code. . import altair as alt import pandas as pd import requests import json import matplotlib.pyplot as plt import math . . Get the data . The following datasets have been used in this post: . Estimated Malaria Deaths | Estimated Malaria Cases | . # get the data def get_data(data_id): r = requests.request(&#39;GET&#39;, &#39;https://ckan.africadatahub.org/api/3/action/datastore_search?resource_id=%s&amp;limit=5000&#39;%(data_id)) c = json.loads(r.content) df = pd.json_normalize(c[&#39;result&#39;][&#39;records&#39;]) #print(df.head(10)) #print(df.Indicator.drop_duplicates()) return df df_deaths = get_data(data_id_deaths) df_cases = get_data(data_id_cases) . . These datasets have 2344 rows with 35 columns each. We&#39;re only interested in a few columns and we&#39;re also only interested in African countries, so we can select the countries and columns as follows. . def cut_data(df): df = df[df.ParentLocation==&#39;Africa&#39;] cols = [&#39;SpatialDimValueCode&#39;, &#39;Location&#39;,&#39;Indicator&#39;,&#39;Period&#39;,&#39;FactValueNumeric&#39;,&#39;FactValueNumericLow&#39;,&#39;FactValueNumericHigh&#39;,&#39;DateModified&#39;] df = df.loc[:,cols] df = df.rename(columns={&#39;FactValueNumeric&#39;:&#39;value&#39;,&#39;FactValueNumericLow&#39;:&#39;low_bound&#39;,&#39;FactValueNumericHigh&#39;:&#39;up_bound&#39;}) print(&quot;New shape: {}&quot;.format(df.shape)) return df df_deaths = cut_data(df_deaths) df_cases = cut_data(df_cases) . . New shape: (927, 8) New shape: (927, 8) . Let&#39;s combine these datasets and explore the data . df = pd.merge(df_deaths,df_cases,on=[&#39;SpatialDimValueCode&#39;,&#39;Location&#39;,&#39;Period&#39;],suffixes=(&quot;_deaths&quot;,&quot;_cases&quot;)) . . #%% create filters locations = df.Location.unique() locations = list(filter(lambda d: d is not None, locations)) # filter out None values locations.sort() # sort alphabetically demo_labels = locations.copy() input_dropdown = alt.binding_select(options=locations, name=&#39;Select country&#39;,labels=demo_labels) selection = alt.selection_single(fields=[&#39;Location&#39;], bind=input_dropdown,init={&#39;Location&#39;:&#39;Kenya&#39;}) . . # Deaths w = 330 # width h = 280 # height title = alt.TitleParams(&#39;Estimated number of deaths due to Malaria in Selected Country&#39;, anchor=&#39;middle&#39;) line = alt.Chart(df,title=title).mark_line().encode( alt.X(&#39;Period:O&#39;,title=&#39;Year&#39;), # :O tells altair that the data is ordinal alt.Y(&#39;value_deaths&#39;,title=&#39;Number of Deaths&#39;) ).properties( width=w, height=h ) #line.show() point = alt.Chart(df).mark_area(opacity=0.3).encode( alt.X(&#39;Period:O&#39;), alt.Y(&#39;low_bound_deaths&#39;), alt.Y2(&#39;up_bound_deaths&#39;), tooltip=[&#39;Period&#39;,&#39;low_bound_deaths&#39;,&#39;value_deaths&#39;,&#39;up_bound_deaths&#39;] ).properties( width=w, height=h ).interactive() # cases title = alt.TitleParams(&#39;Estimated number of cases of Malaria in Selected Country&#39;, anchor=&#39;middle&#39;) line2 = alt.Chart(df,title=title).mark_line().encode( alt.X(&#39;Period:O&#39;,title=&#39;Year&#39;), # :O tells altair that the data is ordinal alt.Y(&#39;value_cases&#39;,title=&#39;Number of Cases&#39;) ).properties( width=w, height=h ) #line.show() point2 = alt.Chart(df).mark_area(opacity=0.3).encode( alt.X(&#39;Period:O&#39;), alt.Y(&#39;low_bound_cases&#39;), alt.Y2(&#39;up_bound_cases&#39;), tooltip=[&#39;Period&#39;,&#39;low_bound_cases&#39;,&#39;value_cases&#39;,&#39;up_bound_cases&#39;] ).properties( width=w, height=h ).interactive() # combine plots x = line + point | line2 + point2 x = x.add_selection( selection ).transform_filter( selection ) x.save(&#39;chart.html&#39;) x . .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/jupyter/malaria/who/data/quicklook/2022/08/12/Malaria_blog_post_altair.html",
            "relUrl": "/jupyter/malaria/who/data/quicklook/2022/08/12/Malaria_blog_post_altair.html",
            "date": " • Aug 12, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post (NOT For Publishing)",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post (NOT For Publishing)",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://heikoheilgendorff.github.io/adh_data_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About This Blog",
          "content": "This blog is a staging point for blog posts on the ADH website. We use this blog as a place to post first drafts of our data blogs, which are created from Jupyter notebooks. This website is powered by fastpages, a blogging platform that natively supports Jupyter notebooks in addition to other formats. From here, we rework the output into Webflow using embeded HTML and then publish on our website. If you have reached this website accidentally, please navigate to https://www.africadatahub.org/ to view the final versions of these posts. .",
          "url": "https://heikoheilgendorff.github.io/adh_data_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://heikoheilgendorff.github.io/adh_data_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}